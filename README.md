# Demo: Robustez de IA contra Ataques Adversariais

Uma aplica√ß√£o web (Flask + PyTorch) que demonstra visualmente como modelos de Deep Learning podem ser enganados por "ataques adversariais" e como o "treinamento adversarial" os torna robustos.

![GIF da aplica√ß√£o em funcionamento](docs/application.gif)

---

## üéØ O Problema: IAs podem ser enganadas

Modelos de vis√£o computacional (CNNs) s√£o incrivelmente poderosos, mas fr√°geis. Uma pequena perturba√ß√£o na imagem, invis√≠vel aos olhos humanos, pode fazer um modelo que classifica um "Avi√£o" com 99% de confian√ßa passar a classific√°-lo como "Navio". Isso √© um **Ataque Adversarial**.

Este projeto demonstra esse fen√¥meno e sua principal solu√ß√£o: o **Treinamento Adversarial**.

## ‚ú® Funcionalidades

* **Upload de Imagem:** Envie qualquer imagem (de prefer√™ncia de uma das classes do CIFAR-10: avi√£o, carro, p√°ssaro, etc.).
* **Compara√ß√£o Lado a Lado:** Veja a predi√ß√£o de dois modelos:
    1.  **Modelo Vulner√°vel:** Uma CNN padr√£o (treinada normalmente).
    2.  **Modelo Robusto:** A mesma CNN, mas "vacinada" com Treinamento Adversarial.
* **Simula√ß√£o de Ataque:** A aplica√ß√£o gera um ataque (FGSM) e mostra como o Modelo Vulner√°vel **falha**, enquanto o Modelo Robusto **resiste**.
* **An√°lise de Confian√ßa:** Visualize o "trade-off" de robustez: o modelo robusto pode ter uma confian√ßa menor em imagens limpas, pois foi treinado para ser mais "cauteloso".

---

## üõ†Ô∏è Stack Tecnol√≥gica

* **Backend (ML & API):**
    * [**Python**](https://www.python.org/)
    * [**PyTorch**](https://pytorch.org/) (Para treinar os modelos e gerar os ataques)
    * [**Flask**](https://flask.palletsprojects.com/) (Para servir a API RESTful)
* **Frontend (UI):**
    * [**HTML5**](https://developer.mozilla.org/pt-BR/docs/Web/HTML)
    * [**CSS3**](https://developer.mozilla.org/pt-BR/docs/Web/CSS)
    * [**JavaScript**](https://developer.mozilla.org/pt-BR/docs/Web/JavaScript) (Puro, para chamadas `fetch` √† API)

---

## üöÄ Como Executar Localmente

Siga estes passos para rodar o projeto na sua m√°quina.

### 1. Pr√©-requisitos

* Python 3.9+
* `pip` (gerenciador de pacotes)

### 2. Instala√ß√£o

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/dicardoso/adversarial_robustness.git
    cd adversarial_robustness
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    # Windows
    python -m venv venv
    .\venv\Scripts\activate

    # Mac/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Instalar as bibliotecas:**
    * No projeto j√° existe o `requirements.txt`, portanto apenas rode:
    ```bash
    pip install -r requirements.txt
    ```

### 3. Treine os Modelos

Antes de rodar o app, voc√™ precisa gerar os arquivos `.pth`.

1.  **Treine o modelo vulner√°vel (padr√£o):**
    ```bash
    python train_vulnerable.py
    ```
    *(Isso ir√° criar `modelo_vulneravel_cifar10.pth`)*

2.  **Treine o modelo robusto:**
    ```bash
    python train_robust.py
    ```
    *(Isso ir√° criar `modelo_robusto_cifar10.pth`)*

### 4. Rode a Aplica√ß√£o

1.  **Inicie o servidor Flask:**
    ```bash
    python app.py
    ```

2.  **Acesse no navegador:**
    Abra seu navegador e v√° para `http://127.0.0.1:5000`

---

## üìñ Conceitos-Chave

* **Ataque FGSM (Fast Gradient Sign Method):** O ataque usado nesta demonstra√ß√£o. Ele calcula o "gradiente" (a dire√ß√£o de maior erro) e d√° um pequeno "empurr√£o" nos pixels da imagem nessa dire√ß√£o, maximizando a chance de o modelo errar.
* **Treinamento Adversarial:** A t√©cnica de defesa. Durante o treinamento, "atacamos" o modelo repetidamente (com FGSM, PGD, etc.) e o ensinamos a classificar corretamente *mesmo* a imagem atacada. √â como uma vacina.

## üåü Pr√≥ximos Passos

A arquitetura atual usa um √∫nico modelo robusto. O pr√≥ximo passo desta pesquisa √© implementar um *ensemble* de modelos e usar t√©cnicas de **sele√ß√£o din√¢mica baseada em incerteza** para otimizar a performance e a robustez.
